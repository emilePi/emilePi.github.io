<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Discretization Schemes | Emile Pierret</title>
    <link>http://localhost:1313/tag/discretization-schemes/</link>
      <atom:link href="http://localhost:1313/tag/discretization-schemes/index.xml" rel="self" type="application/rss+xml" />
    <description>Discretization Schemes</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en</language><copyright>2020</copyright><lastBuildDate>Thu, 01 May 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/images/icon_hu74e2ef43be67154c1e224a044141f041_254118_512x512_fill_lanczos_center_3.png</url>
      <title>Discretization Schemes</title>
      <link>http://localhost:1313/tag/discretization-schemes/</link>
    </image>
    
    <item>
      <title>Diffusion models for Gaussian distributions: Exact solutions and Wasserstein errors</title>
      <link>http://localhost:1313/publication/w2_gaussian/</link>
      <pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/w2_gaussian/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;We keep the same equation numbering as in the paper.&lt;br&gt;
We study the continuous formulation of diffusion models, focusing on the Variance Preserving (VP-SDE) framework 
&lt;a href=&#34;#Song&#34;&gt;[Song et al., 2021]&lt;/a&gt;. The forward process is:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\label{eq:sde_forward}
d\boldsymbol{x}_t  =- \beta_t \boldsymbol{x}_t dt + \sqrt{2\beta_t}d\boldsymbol{w}_t, \quad 0 \leq t \leq T,   \boldsymbol{x}_0 \sim p_{\text{data}}. \tag{1}
\end{equation}&lt;/p&gt;
&lt;p&gt;This degrades the data progressively. Denoting by ( p_t ) the marginal distribution of ( \boldsymbol{x}_t ), we have ( p_T \approx \mathcal{N}(0, I) ) if ( \beta_t ) is properly chosen.&lt;br&gt;
Theoretically, the reverse-time process 
&lt;a href=&#34;#Pardoux&#34;&gt;[Pardoux, 1986]&lt;/a&gt; is governed by the backward SDE:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\label{eq:sde_backward}
d{\boldsymbol{y}}_t = \beta_{T-t}[{\boldsymbol{y}}_t + 2  \nabla \log  p_{T-t}({\boldsymbol{y}}_t)]dt + \sqrt{2\beta_{T-t}}d{\boldsymbol{w}}_t,
\quad 0 \leq t &amp;lt; T, {\boldsymbol{y}}_0 \sim p_T \tag{2}
\end{equation}&lt;/p&gt;
&lt;p&gt;Or equivalently, the associated probability flow ODE:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\label{eq:flow_reverse_ode}
d{\boldsymbol{y}}_t = \beta_{T-t}\left[{\boldsymbol{y}}_t+ \nabla\log p_{T-t}({\boldsymbol{y}}_t)\right]dt, \quad 0 \leq t &amp;lt; T, {\boldsymbol{y}}_0 \sim p_T. \tag{5}
\end{equation}&lt;/p&gt;
&lt;p&gt;We illustrate this idea below: assuming that ( p_{\text{data}} ) is Gaussian (left of the figure), the forward process evolves from left to right. The &lt;span style=&#34;color: rgb(142,105,185);&#34;&gt;backward SDE (purple)&lt;/span&gt; and the probability flow ODE (black) reverse this evolution, with overlapping Gaussian ellipses in blue at each time step.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;true.png&#34; alt=&#34;Theory&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;error-types-in-diffusion-models&#34;&gt;Error Types in Diffusion Models&lt;/h2&gt;
&lt;p&gt;We identify four distinct sources of error:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Initialization error&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Truncation error&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Discretization error&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Score approximation error&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;initialization-error&#34;&gt;Initialization Error&lt;/h3&gt;
&lt;p&gt;Occurs when the backward SDE or flow ODE is initialized with ( \mathcal{N}(0,I) ) instead of the true ( p_T ). This mismatch arises because ( p_T ) is unknown in general. Although the forward process converges to ( \mathcal{N}(0, I) ) asymptotically, in practice we must use a finite ( T ). This mismatch causes divergence between the forward and reverse marginals.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;init.png&#34; alt=&#34;Initialization&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;truncation-error&#34;&gt;Truncation Error&lt;/h3&gt;
&lt;p&gt;This error results from stopping the reverse process at some small ( \varepsilon &amp;gt; 0 ) instead of time 0. It&amp;rsquo;s necessary when the data distribution lacks a density, e.g. if it&amp;rsquo;s supported on a manifold, making ( \nabla \log p_t ) undefined at ( t=0 ).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;truncation.png&#34; alt=&#34;Truncation&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;discretization-error&#34;&gt;Discretization Error&lt;/h3&gt;
&lt;p&gt;Discretizing the backward SDE or flow ODE is required for numerical simulation. The choice of discretization scheme (Euler, Heun, etc.) directly impacts the quality of the generated samples.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;discrete.png&#34; alt=&#34;Discretization&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;score-approximation-error&#34;&gt;Score Approximation Error&lt;/h3&gt;
&lt;p&gt;In practice, the score function ( \nabla \log p_t ) is unknown and approximated by a neural network ( s_\theta ). This introduces a model approximation error. The success of diffusion models hinges on the quality of this learned score function.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;analysis-under-gaussian-assumption&#34;&gt;Analysis Under Gaussian Assumption&lt;/h2&gt;
&lt;p&gt;To precisely analyze these errors, we focus on a tractable case:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumption (Gaussian assumption):&lt;/strong&gt;  $\quad  p_{\text{data}} = \mathcal{N}(0, \boldsymbol{\Sigma}) $&lt;/p&gt;
&lt;p&gt;This allows closed-form derivations. The covariance matrix ( \boldsymbol{\Sigma} ) may be non-invertible, in which case the data lies on a manifold.&lt;/p&gt;
&lt;p&gt;The exact score is given by:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\nabla_{\boldsymbol{x}} \log p_t(\boldsymbol{x}) = -\boldsymbol{\Sigma}_t^{-1} \boldsymbol{x}
\end{equation}&lt;/p&gt;
&lt;p&gt;where $ \boldsymbol{\Sigma}_t = e^{-\int_0^t \beta_s ds} \boldsymbol{\Sigma} + (1 - e^{-\int_0^t \beta_s ds}) I $.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; When $\boldsymbol{\Sigma} $ is singular, the score is undefined at ( t = 0 ), explaining the necessity of truncation.&lt;/p&gt;
&lt;p&gt;In this Gaussian setting, we:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Derive closed-form solutions for the backward SDE (&lt;strong&gt;Proposition 2&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Solve the probability flow ODE exactly (&lt;strong&gt;Proposition 3&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Compute exact Wasserstein-2 errors from initialization, truncation, and discretization (&lt;strong&gt;Section 4&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Empirically study the score approximation error (&lt;strong&gt;Section 5&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;key-takeaways&#34;&gt;Key Takeaways&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;With only initialization error, the ODE sampler is more accurate than the SDE sampler (&lt;strong&gt;Proposition 4&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Adding truncation and discretization errors, Heun&amp;rsquo;s method applied to the ODE yields the best performance (&lt;strong&gt;Figure 1&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Score approximation is the most critical error in practice, and the SDE sampler is more robust to this noise&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For full derivations and results, see the paper.&lt;/p&gt;
&lt;details&gt; 
&lt;summary&gt;Bibtex&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{Pierret_Galerne_diffusion_models_Gaussian_exact_solutions_errors_ICML2025,
title={Diffusion models for Gaussian distributions: Exact solutions and Wasserstein errors},
author={Emile Pierret and Bruno Galerne},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
 &lt;p id=&#34;Song&#34;&gt; &lt;span style=&#34;color: blue;&#34;&gt;[Song et al., 2021]&lt;/span&gt;  Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, &amp; Ben Poole (2021). Score-Based Generative Modeling through Stochastic Differential Equations. In International Conference on Learning Representations. &lt;/p&gt;
&lt;p id=&#34;Pardoux&#34;&gt; &lt;span style=&#34;color: blue;&#34;&gt;[Pardoux, 1986]&lt;/span&gt; Pardoux, Étienne. Grossissement d&#39;une filtration et retournement du temps d&#39;une diffusion. Séminaire de probabilités, Tome 20 (1986), pp. 48-55. &lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
